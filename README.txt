README File

Team Name: The Bullet Club
Authors: Iveri Prangishvili, Rishu Agrawal, Neerav Karani

Included are two different sentiment analysis models:
Convolutional Neural Network (CNN) and LSTM-GRU model.
On kaggle selected submission files are:
1. submission_CNN.csv (CNN model)
2. submission_LSTM_GRU.csv (LSTM-GRU model)

included files:
1. CNN (dir)
  1. tensorCNN.py
  2. batchData.py
  3. run.py
  4. 1466591113 (dir)
    1. chekpoints (dir)
      1. model-136000

2. LSTM_GRU_keras (dir)
  1. kerasLSTMGRU.py
  2. run.py
  3. LSTM_GRU_architecture.json
  4. LSTM_GRU_weights_ep2_100.h5

3. data_padding.py
3. glove.py
4. word2vec.py
5. twitter-datasets (dir)
  1. testX.npy
6. word2vec_data (dir)
  1. emb_100.npy
7. modelData (dir)
  1. embeddings_nostop_40_sum_pad.npy
8. pickle_vocab.py
9. README.txt



File descriptions ==============================================================

data_padding.py - load twitter data, pad sentences to 31 words. generate numpy arrays:
training data (combined positive and negative sets), labels, test data.
saves them in twitter-datasets directory.

glove.py - implementation of Glove embeddings (runs on small dataset). saves embeddings as
a numpy array in modelData directory

word2vec.py - implementation of Word2Vec (runs on large dataset). Saves embeddings
as numpy array in modelData directory

pickle_vocab.py - reads vocub_cut.txt file, generated using shell scripts provided
with the project. Creates vocabulary from the file, adds vocabulary ID for <PAD/>
(specific tag used to pad sentences to fixed length) and saves it as a pickle file.

batchData.py (CNN directory)- implementation of functions for generating batches of data.

tensorCNN.py (CNN directory) - implementation of CNN for text classification. Trains CNN, writes
train/test summary, saves checkpoint of models for later use (training/prediction)

run.py (CNN directory) - loads specified CNN model, and runs it on test data. Generates predictions
as submission_CNN.csv file.

kerasLSTMGRU.py (LSTM_GRU_keras dir) - implements LSTM_GRU model using keras. model is composed of
following layers: input layer, Embedding layer (initialized with word2vec embeddings), LSTM layer,
GRU forward / GRU backward, softmax layer, output layer. Program runs on training data and evaluates once every
epoch. Saves model architecture and weights once finished training.

run.py (LSTM_GRU_keras dir) - loads specified LSTM_GRU model architecture and weights. Runs
it on test data and generates predictions as submission_LSTM_GRU.csv

================================================================================
How to generate prediction files:

Convolutional Neural Network (CNN) model:
1. go to CNN directory and run: python3 run.py
It will generate submission_CNN.csv file.

LSTM_GRU model:
1. go to LSTM_GRU_keras directory and run: python3 run.py
it will generate submission_LSTM_GRU.csv file.


================================================================================
How to reproduce results for Convolutional Neural Network (CNN) model:

follow this order:

1. run data_padding.py to generate numpy arrays of train, labels, test data. (will need to
change path to vocabulary file and training files manually)

2. run glove.py to generate word embeddings or skip this step and use
already generated embeddings in modelData directory
(will need to change path to cooc file - generated by cooc.py program - manually)

3. run word2vec.py to generate word embeddings or skip this step and use
already generated embeddings in word2vec_data directory
(change path to vocabulary file and training files manually)

4. run tensorCNN.py to train the model and generate checkpoints. (will take a while to run)

5. run run.py (CNN directory) to generate prediction file (specify the saved model to use for prediction)
otherwise will run on default model checkpoint

================================================================================
How to reproduce results for LSTM_GRU model:

follow this order:

1. install Keras, HDF5 for python (run keras on tensorflow - backend)

2. run data_padding.py to generate numpy arrays of train, labels, test data. (will need to
change path to vocabulary file and training files manually)

3. run word2vec to generate 100 dimensional word embeddings (or skip this step and use
already generated embeddings in word2vec_data directory)

4. run kerasLSTMGRU.py to train the model and generate model architecture and save model weights.

5. run run.py (LSTM_GRU_keras directory) - to generate prediction file (specify the model weights to use
for prediction otherwise will run on default model weights.)
